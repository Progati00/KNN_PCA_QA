# This project presents a comprehensive exploration of both theoretical and practical aspects of K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA). It begins with a detailed set of theoretical Q&A covering essential concepts, including how KNN works, differences between classification and regression, the role of distance metrics, curse of dimensionality, PCA mechanics, eigenvalues and eigenvectors, feature selection vs. extraction, and more. Each question is answered with clarity to build a strong conceptual understanding.

# The practical section complements the theory with hands-on coding tasks using Python and scikit-learn. It includes training KNN classifiers and regressors, analyzing performance using metrics like accuracy, MSE, ROC-AUC, and F1-score, experimenting with different distance metrics and values of K, and visualizing decision boundaries. On the PCA side, tasks focus on dimensionality reduction, explained variance analysis, reconstruction error visualization, and integrating PCA with KNN to compare performance. Advanced tasks include hyperparameter tuning using GridSearchCV and handling missing data using KNN imputation.
